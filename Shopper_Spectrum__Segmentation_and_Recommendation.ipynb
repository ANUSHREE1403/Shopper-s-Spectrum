{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANUSHREE1403/Shopper-s-Spectrum/blob/main/Shopper_Spectrum__Segmentation_and_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name -** Shopper Spectrum: Customer Segmentation and Product Recommendations in E-Commerce  \n",
        "# **Project Type -** Unsupervised + Recommendation System  \n",
        "# **Contribution -** Individual  \n",
        "# Team Member 1 - Anushree"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:** Segment e-commerce users based on behavioral patterns to support personalized marketing and customer engagement.\n",
        "\n",
        "**Dataset Used:** Online retail customer behavior dataset (classification-based segmentation).\n",
        "\n",
        "**Tech Stack:** Python, Scikit-learn, LightGBM, Random Forest, Streamlit.\n",
        "\n",
        "**Process:**\n",
        "\n",
        "Performed EDA using the UBM (Univariate-Bivariate-Multivariate) framework.\n",
        "\n",
        "Handled outliers, missing values, and imbalanced data.\n",
        "\n",
        "Applied feature engineering and label encoding.\n",
        "\n",
        "Built and compared ML models with hyperparameter tuning (RandomSearchCV).\n",
        "\n",
        "**Outcome:** Achieved >99.9% accuracy with LightGBM model.\n",
        "\n",
        "**Deliverables:**\n",
        "\n",
        "Best model saved as .pkl file.\n",
        "\n",
        "Streamlit app for real-time predictions via form or CSV upload.\n",
        "\n",
        "Encapsulated ML pipeline ready for deployment."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ANUSHREE1403/Shopper-s-Spectrum"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dynamic world of e-commerce, understanding customer behavior is crucial for driving growth and retention. Businesses often struggle to identify which customers are most valuable, which ones are at risk of churning, and how to target each segment effectively. Without proper segmentation, marketing campaigns become generic, customer experiences suffer, and profitability declines.\n",
        "\n",
        "This project aims to build a machine learning-based solution to segment e-commerce customers based on their purchasing behavior. By classifying customers into meaningful groups, the platform can:\n",
        "\n",
        "Personalize marketing strategies,\n",
        "\n",
        "Improve customer retention, and\n",
        "\n",
        "Maximize sales and customer lifetime value."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# For clustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# For recommendations\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# For deployment\n",
        "import joblib\n",
        "\n",
        "print(\" Libraries successfully imported!\")\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the uploaded file as CSV with correct encoding\n",
        "df = pd.read_csv('/content/online_retail.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Show the first 5 rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3VMDY-Ok5M-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Dataset Rows & Columns count\n",
        "print(\"Number of Rows:\", df.shape[0])\n",
        "print(\"Number of Columns:\", df.shape[1])"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Dataset Information\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Dataset Description\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "PVblCFt4T55t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Null values count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Checking and removing duplicate rows\n",
        "print(\"Before removing duplicates:\", df.shape)\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "print(\"After removing duplicates:\", df.shape)"
      ],
      "metadata": {
        "id": "F183genVUVRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Visualizing null values using heatmap\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Dimensions**\n",
        "\n",
        "The dataset contains a large number of rows and 8 columns.\n",
        "\n",
        "**Columns and Their Meaning**\n",
        "\n",
        "invoiceno: Unique identifier for each invoice/transaction.\n",
        "\n",
        "stockcode: Product code.\n",
        "\n",
        "description: Text description of the product.\n",
        "\n",
        "quantity: Number of units sold. Can be negative (indicates returns).\n",
        "\n",
        "invoicedate: Date and time of the transaction.\n",
        "\n",
        "unitprice: Price per item.\n",
        "\n",
        "customerid: ID of the customer. Missing for some rows.\n",
        "\n",
        "country: Country where the transaction occurred.\n",
        "\n",
        "**Missing Values**\n",
        "\n",
        "The customerid column has missing values.\n",
        "\n",
        "The description column also has some missing values.\n",
        "\n",
        "These were visualized using a heatmap.\n",
        "\n",
        "**Duplicates**\n",
        "\n",
        "Duplicate rows existed and were successfully removed.\n",
        "\n",
        "**Data Types**\n",
        "\n",
        "Most columns are of type object or float.\n",
        "\n",
        "The invoicedate column will need to be converted to datetime format.\n",
        "\n",
        "**Cleaned Column Names**\n",
        "\n",
        "All column names have been stripped of spaces and converted to lowercase with underscores for consistency.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. View column names\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Rename columns for consistency\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# View renamed columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "07B7JEzXVPD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique counts of important categorical columns\n",
        "print(\"Unique Invoice Numbers:\", df['invoiceno'].nunique())\n",
        "print(\"Unique Stock Codes:\", df['stockcode'].nunique())\n",
        "print(\"Unique Product Descriptions:\", df['description'].nunique())\n",
        "print(\"Unique Customers:\", df['customerid'].nunique())\n",
        "print(\"Unique Countries:\", df['country'].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'invoicedate' to datetime format\n",
        "df['invoicedate'] = pd.to_datetime(df['invoicedate'])\n",
        "\n",
        "# Show updated data types\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components from datetime\n",
        "df['year'] = df['invoicedate'].dt.year\n",
        "df['month'] = df['invoicedate'].dt.month\n",
        "df['day'] = df['invoicedate'].dt.day\n",
        "df['hour'] = df['invoicedate'].dt.hour\n",
        "df['dayofweek'] = df['invoicedate'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "\n",
        "# Show the first few rows with new time columns\n",
        "df[['invoicedate', 'year', 'month', 'day', 'hour', 'dayofweek']].head()"
      ],
      "metadata": {
        "id": "Nh5JKmtcVtvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#total sales per month\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create 'total_price' column\n",
        "df['total_price'] = df['quantity'] * df['unitprice']\n",
        "\n",
        "# Group by Year and Month\n",
        "monthly_sales = df.groupby(['year', 'month'])['total_price'].sum().reset_index()\n",
        "\n",
        "# Create a 'year-month' column for plotting\n",
        "monthly_sales['year_month'] = monthly_sales['year'].astype(str) + '-' + monthly_sales['month'].astype(str).str.zfill(2)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(monthly_sales['year_month'], monthly_sales['total_price'], marker='o')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Total Sales per Month\")\n",
        "plt.xlabel(\"Year-Month\")\n",
        "plt.ylabel(\"Total Sales (£)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Why did you pick the specific chart?**\n",
        "A line chart is ideal for visualizing trends over time. It shows how sales change from month to month, highlighting seasonality and growth patterns clearly.\n",
        "\n",
        "**2. What is/are the insight(s) found from the chart?**\n",
        "The chart reveals the monthly revenue trend, peaks in certain months, and possible seasonal effects. It may also help identify months with lower sales or potential promotional opportunities.\n",
        "\n",
        "**3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "Yes, these insights can help plan marketing campaigns during low-performing months and prepare inventory for peak months, which will positively impact revenue. If certain months consistently underperform, the business might investigate reasons (like supply issues or lower demand) and take corrective action."
      ],
      "metadata": {
        "id": "vG_n1Sw0WQEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2: Top 10 Countries by Total Sales\n",
        "\n",
        "# Grouping by country and summing total sales\n",
        "top_countries = df.groupby('country')['total_price'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,6))\n",
        "top_countries.plot(kind='bar', color='skyblue')\n",
        "plt.title(\"Top 10 Countries by Total Sales\")\n",
        "plt.ylabel(\"Total Sales (£)\")\n",
        "plt.xlabel(\"Country\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "A bar chart is appropriate here because it compares discrete categories (countries) based on total revenue. It provides a clear visual hierarchy of which countries contribute the most to the business.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "The chart shows that the UK dominates the total sales, followed by other European countries. It highlights which regions are the company’s strongest markets.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Yes, these insights can guide the business to focus marketing efforts on high-performing countries and investigate ways to improve performance in lower-ranked countries. If some countries show unusually low sales despite potential demand, this could indicate missed opportunities or distribution issues."
      ],
      "metadata": {
        "id": "seBnsqbGWsWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Chart - 3: Top 10 Most Sold Products (by quantity)\n",
        "\n",
        "top_products = df.groupby('description')['quantity'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,6))\n",
        "top_products.plot(kind='bar', color='orange')\n",
        "plt.title(\"Top 10 Most Sold Products\")\n",
        "plt.ylabel(\"Total Quantity Sold\")\n",
        "plt.xlabel(\"Product Description\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "A bar chart is ideal to compare product sales volume across categories. It allows for quick identification of top-selling products by quantity.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "We can clearly identify which products are best-sellers. These may be products that are consistently in demand, and likely to be driving a significant portion of revenue.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Yes — identifying the top-selling products helps optimize inventory, forecast demand, and run targeted promotions. If some best-sellers have thin profit margins or high return rates (explored later), they might negatively affect overall profitability despite high sales."
      ],
      "metadata": {
        "id": "tef3tqgoW3Qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4: Top 10 Most Valuable Products (by total revenue)\n",
        "\n",
        "top_valuable_products = df.groupby('description')['total_price'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,6))\n",
        "top_valuable_products.plot(kind='bar', color='green')\n",
        "plt.title(\"Top 10 Most Valuable Products\")\n",
        "plt.ylabel(\"Total Revenue (£)\")\n",
        "plt.xlabel(\"Product Description\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "Bar charts clearly showcase comparisons among product categories based on revenue. This helps prioritize products that bring in the most money.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "We identify which products contribute the most revenue — not just by sales volume, but by price x quantity. A product may not be sold the most, but could still top revenue.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Yes — this helps in profit optimization by focusing on high-revenue products. If a high-revenue product also has high return rates, stockouts, or customer complaints, it might negatively impact long-term business, requiring deeper review."
      ],
      "metadata": {
        "id": "wXNlhhinXBOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5: Top 10 Customers by Total Revenue\n",
        "\n",
        "top_customers = df.groupby('customerid')['total_price'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,6))\n",
        "top_customers.plot(kind='bar', color='purple')\n",
        "plt.title(\"Top 10 Customers by Revenue\")\n",
        "plt.ylabel(\"Total Revenue (£)\")\n",
        "plt.xlabel(\"Customer ID\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "A bar chart efficiently ranks customers based on the revenue they generate, giving a snapshot of your most valuable clients.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "The chart shows which customers contribute the most to revenue. These could be loyal customers, resellers, or bulk buyers.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Yes — these insights can guide loyalty programs, personalized marketing, or early access strategies. However, over-reliance on a few key customers can be risky if one leaves or reduces spending."
      ],
      "metadata": {
        "id": "8-wk6_6oXLgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6: Monthly Revenue Trend\n",
        "\n",
        "monthly_sales = df.resample('M', on='invoicedate')['total_price'].sum()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12,6))\n",
        "monthly_sales.plot(marker='o', linestyle='-')\n",
        "plt.title(\"Monthly Revenue Trend\")\n",
        "plt.ylabel(\"Total Revenue (£)\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "A time series line chart is the best choice to visualize trends across months. It captures patterns, seasonality, and anomalies in revenue flow.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "You may observe peaks around November/December — likely due to holiday season spikes. Some months may show dips indicating low customer activity or supply chain issues.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Yes — understanding monthly patterns supports seasonal planning, staffing, and promotional timing. A sudden decline in expected months may signal underlying issues like poor campaigns or product unavailability, prompting corrective action."
      ],
      "metadata": {
        "id": "qZhsWhHR5o__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7: Total Value of Returns (Negative Quantity) by Country\n",
        "\n",
        "returns_df = df[df['quantity'] < 0]\n",
        "returns_by_country = returns_df.groupby('country')['total_price'].sum().sort_values().head(10)  # least 10 countries by return value\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,6))\n",
        "returns_by_country.plot(kind='barh', color='red')\n",
        "plt.title(\"Top 10 Countries by Return Value\")\n",
        "plt.xlabel(\"Total Return Value (£)\")\n",
        "plt.ylabel(\"Country\")\n",
        "plt.grid(axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "A horizontal bar chart helps us compare return amounts across countries clearly, especially when country names vary in length.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "We can see which countries experience the most monetary loss due to product returns. This may indicate dissatisfaction, shipping issues, or cultural return behaviors.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Yes — high return values can flag operational or quality problems in specific regions. Reducing returns through quality checks, better logistics, or localized support directly improves profit margins and customer satisfaction.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEY1yBIG5uZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8: Heatmap of Purchases by Hour and Day of Week\n",
        "\n",
        "# Extract hour and weekday from invoice datetime\n",
        "df['hour'] = df['invoicedate'].dt.hour\n",
        "df['weekday'] = df['invoicedate'].dt.day_name()\n",
        "\n",
        "# Create pivot table\n",
        "hourly_pivot = df.pivot_table(index='weekday', columns='hour', values='invoiceno', aggfunc='count').fillna(0)\n",
        "\n",
        "# Reorder weekdays\n",
        "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "hourly_pivot = hourly_pivot.reindex(weekday_order)\n",
        "\n",
        "# Plotting heatmap\n",
        "plt.figure(figsize=(15,6))\n",
        "sns.heatmap(hourly_pivot, cmap='YlGnBu')\n",
        "plt.title('Heatmap of Purchase Volume by Hour and Day of Week')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Day of Week')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "A heatmap helps visualize two-dimensional patterns — in this case, when during the day and which weekdays customers are most active. This is hard to interpret from raw data alone.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "We can identify peak purchasing hours (e.g., mid-morning or early afternoon) and busiest days (e.g., Tuesdays or Thursdays). Lulls may be seen during weekends or night hours.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Yes — these insights support marketing timing, staffing, and promotions. If weekends are unusually slow, it may indicate poor weekend engagement or need for special campaigns."
      ],
      "metadata": {
        "id": "uurr927G6DHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9: Boxplots for Outlier Detection in Quantity and UnitPrice\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Quantity\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(y=df['quantity'])\n",
        "plt.title(\"Boxplot of Quantity\")\n",
        "\n",
        "# UnitPrice\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=df['unitprice'])\n",
        "plt.title(\"Boxplot of UnitPrice\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0XqytLze6qeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "Boxplots clearly highlight outliers and spread of data, making them perfect for numerical outlier detection.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "There are several unusually high values in both quantity and unitprice which deviate far from the norm, indicating potential data entry errors, bulk orders, or anomalies.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Yes. Removing or capping these extreme outliers ensures robust models and reliable insights. If left untreated, they could mislead pricing strategies or inventory planning."
      ],
      "metadata": {
        "id": "UOOSow0d6tqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy original DataFrame\n",
        "df_cleaned = df.copy()\n",
        "\n",
        "# Function to remove outliers using IQR\n",
        "def remove_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    return data[(data[column] >= lower) & (data[column] <= upper)]\n",
        "\n",
        "# Remove outliers from 'quantity' and 'unitprice'\n",
        "df_cleaned = remove_outliers_iqr(df_cleaned, 'quantity')\n",
        "df_cleaned = remove_outliers_iqr(df_cleaned, 'unitprice')\n",
        "\n",
        "# Check new shape\n",
        "print(\"Original rows:\", df.shape[0])\n",
        "print(\"Cleaned rows:\", df_cleaned.shape[0])"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick this method?\n",
        "The IQR method is simple yet effective for identifying extreme values that fall outside a reasonable range in skewed distributions.\n",
        "\n",
        "2. What is/are the insight(s) found from cleaning?\n",
        "Outlier removal reduced the dataset size, removing rare and possibly erroneous or bulk order values that could distort analysis.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth?\n",
        "Yes, cleaning improves model performance and decision-making reliability. There's no negative growth, but ensure legitimate high-value orders are not unintentionally removed."
      ],
      "metadata": {
        "id": "L5D2nwq265EG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H₀ (Null Hypothesis): Mean revenue for UK and non-UK customers is equal.\n",
        "\n",
        "H₁ (Alternative Hypothesis): Mean revenue for UK and non-UK customers is not equal."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Group data by country\n",
        "uk_revenue = df_cleaned[df_cleaned['country'] == 'United Kingdom']['total_price']\n",
        "non_uk_revenue = df_cleaned[df_cleaned['country'] != 'United Kingdom']['total_price']\n",
        "\n",
        "# Perform Welch's t-test\n",
        "t_stat, p_value = ttest_ind(uk_revenue, non_uk_revenue, equal_var=False)\n",
        "\n",
        "print(\"T-statistic:\", round(t_stat, 3))\n",
        "print(\"P-value:\", round(p_value, 3))\n",
        "\n",
        "# Interpret result\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. Revenue differs significantly between UK and non-UK customers.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. No significant difference in revenue.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Welch’s t-test, which is a variation of the independent t-test. This test compares the means of two independent groups, in our case, UK vs. non-UK customer revenues,while accounting for unequal variances and sample sizes between the two groups. The p-value obtained from this test tells us whether the difference in means is statistically significant."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Welch’s t-test was chosen because:\n",
        "\n",
        "The UK and non-UK groups are independent (i.e., different customers from different countries).\n",
        "\n",
        "The sample sizes and variances between the groups are not equal, which violates assumptions of the standard t-test.\n",
        "\n",
        "Welch’s t-test is robust and more reliable in such cases, and is a preferred test when comparing two population means with unequal variance."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H₀ (Null Hypothesis): There is no correlation between quantity and total_price.\n",
        "\n",
        "H₁ (Alternative Hypothesis): There is a significant correlation between quantity and total_price."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Group by 'invoiceno' and sum 'quantity' and 'total_price'\n",
        "invoice_summary = df_cleaned.groupby('invoiceno')[['total_price', 'quantity']].sum().reset_index()\n",
        "\n",
        "# Perform Pearson correlation test\n",
        "corr_coef, p_value = pearsonr(invoice_summary['quantity'], invoice_summary['total_price'])\n",
        "\n",
        "# Show results\n",
        "print(f\"Correlation Coefficient: {corr_coef}\")\n",
        "print(f\"P-Value: {p_value}\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the Pearson Correlation Coefficient test to evaluate the strength and significance of the linear relationship between quantity and total_price per invoice. The p-value obtained from this test tells us whether the correlation is statistically significant."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pearson correlation test is suitable because:\n",
        "\n",
        "Both variables (quantity and total_price) are continuous numerical variables.\n",
        "\n",
        "We want to test for a linear relationship between these variables.\n",
        "\n",
        "It provides both the correlation coefficient (strength & direction) and the p-value to determine significance."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Check missing values\n",
        "missing_values = df_cleaned.isnull().sum()\n",
        "missing_percent = (missing_values / len(df_cleaned)) * 100\n",
        "\n",
        "# Combine into a DataFrame for better visibility\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Percentage': missing_percent\n",
        "}).sort_values(by='Percentage', ascending=False)\n",
        "\n",
        "missing_df"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CustomerID column had missing values.\n",
        "\n",
        "Since CustomerID is a categorical identifier and not useful for modeling or aggregation directly, we dropped the rows where it was missing.\n",
        "\n",
        "We did not use imputation (like mode or random ID) to avoid data leakage or incorrect grouping.\n",
        "\n",
        "For other columns, there were no missing values, so no imputation was needed."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizing outliers for 'quantity' and 'unitprice'\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x=df_cleaned['quantity'])\n",
        "plt.title(\"Boxplot - Quantity\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x=df_cleaned['unitprice'])\n",
        "plt.title(\"Boxplot - UnitPrice\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers were identified in the quantity and unitprice columns using boxplots.\n",
        "\n",
        "We used the Interquartile Range (IQR) method to detect and treat extreme outliers.\n",
        "\n",
        "Rows where:\n",
        "\n",
        "quantity was unusually high/low (like negative or above 99th percentile)\n",
        "\n",
        "or unitprice was negative or extremely high\n",
        "\n",
        "were removed.\n",
        "\n",
        "Reason:\n",
        "\n",
        "Negative quantity values usually indicate returns or cancellations.\n",
        "\n",
        "Negative unitprice values are likely data errors.\n",
        "\n",
        "Removing them improves the reliability of analysis and modeling."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Check object/categorical columns\n",
        "df_cleaned.select_dtypes(include='object').nunique()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding for binary or low-cardinality categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Making a copy of the dataset\n",
        "df_encoded = df_cleaned.copy()\n",
        "\n",
        "# Encoding 'country' using LabelEncoder since it's not used for modeling but helpful as a numeric feature\n",
        "le = LabelEncoder()\n",
        "df_encoded['country_encoded'] = le.fit_transform(df_encoded['country'])\n",
        "\n",
        "# Dropping the original 'country' to avoid redundancy\n",
        "df_encoded.drop('country', axis=1, inplace=True)\n",
        "\n",
        "# Checking the result\n",
        "df_encoded.head()\n"
      ],
      "metadata": {
        "id": "pNdpfW6XCwNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Label Encoding for the country column.\n",
        "\n",
        "Reason:\n",
        "\n",
        "Label encoding assigns each unique category a numeric code.\n",
        "\n",
        "Since we're not using country for categorical relationships or model decision paths (like in trees), label encoding is fast and efficient.\n",
        "\n",
        "The column had moderate cardinality and was not intended for detailed categorical analysis.\n",
        "\n",
        "We avoided One-Hot Encoding as it would create many dummy columns due to the large number of unique countries, leading to dimensionality explosion."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Example: Extracting additional features from 'invoicedate'\n",
        "df_encoded['is_weekend'] = df_encoded['dayofweek'].apply(lambda x: 1 if x in [5,6] else 0)\n",
        "df_encoded['is_night'] = df_encoded['hour'].apply(lambda x: 1 if x < 6 or x > 20 else 0)\n",
        "\n",
        "# Preview the new features\n",
        "df_encoded[['dayofweek', 'hour', 'is_weekend', 'is_night']].head()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Select only numeric columns for correlation\n",
        "numeric_df = df_encoded.select_dtypes(include='number')\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derived is_weekend from dayofweek to identify whether the transaction happened during the weekend.\n",
        "\n",
        "Derived is_night from hour to capture user behavior during late hours.\n",
        "\n",
        "These binary features may help improve model performance by capturing user purchase patterns across time segments."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation was necessary because the distribution of some numerical features like unitprice, quantity, and total_price was highly skewed (right-skewed). Highly skewed data can adversely affect model performance, especially for models that assume normal distribution (e.g., linear regression, logistic regression)."
      ],
      "metadata": {
        "id": "OD2g0s6hGQ8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformation Technique Used:\n",
        "Log Transformation\n",
        "Applied to: unitprice, quantity, and total_price\n",
        "Reason: To reduce skewness and normalize the distribution of numerical features, improving model performance and convergence."
      ],
      "metadata": {
        "id": "JccF07D9GVEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Log transform skewed numerical columns\n",
        "df_encoded['log_unitprice'] = np.log1p(df_encoded['unitprice'])\n",
        "df_encoded['log_quantity'] = np.log1p(df_encoded['quantity'])\n",
        "df_encoded['log_total_price'] = np.log1p(df_encoded['total_price'])\n",
        "\n",
        "# Drop the original columns if not needed\n",
        "# df_encoded.drop(['unitprice', 'quantity', 'total_price'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Check for infinite values\n",
        "print(np.isinf(df_encoded[numeric_cols]).sum())\n",
        "\n",
        "# Replace inf/-inf with NaN\n",
        "df_encoded[numeric_cols] = df_encoded[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Then handle the NaNs (e.g., with mean imputation or dropping)\n",
        "df_encoded[numeric_cols] = df_encoded[numeric_cols].fillna(df_encoded[numeric_cols].mean())\n",
        "\n",
        "# Now apply scaling\n",
        "df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Standard Scaling (Z-score normalization) to scale our features."
      ],
      "metadata": {
        "id": "0qlK24KXGhZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many ML models (e.g., KNN, SVM, Logistic Regression) are sensitive to the scale of the data.\n",
        "\n",
        "StandardScaler transforms the features such that they have a mean of 0 and standard deviation of 1.\n",
        "\n",
        "It's effective when the data is normally distributed or approximately so (after log transformation)"
      ],
      "metadata": {
        "id": "3RvHiyPeGkG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction can be helpful in this case:\n",
        "\n",
        "It helps reduce multicollinearity by removing redundant features.\n",
        "\n",
        "It improves model performance by eliminating noise and irrelevant information.\n",
        "\n",
        "It enables faster computation and better visualization in lower dimensions."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize PCA to retain 95% of the variance\n",
        "pca = PCA(n_components=0.95)\n",
        "\n",
        "# Apply PCA on scaled numeric columns\n",
        "pca_features = pca.fit_transform(df_encoded[numeric_cols])\n",
        "\n",
        "# Create a DataFrame for the reduced features\n",
        "pca_df = pd.DataFrame(pca_features, columns=[f'PC{i+1}' for i in range(pca_features.shape[1])])"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Principal Component Analysis (PCA) because:\n",
        "\n",
        "PCA helps in capturing the maximum variance with the fewest number of components.\n",
        "\n",
        "It transforms features into orthogonal (independent) components.\n",
        "\n",
        "It's suitable for continuous numerical data, like the scaled log_ features."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid overfitting and to ensure our model generalizes well:\n",
        "\n",
        "Training Set: Used to train the model.\n",
        "\n",
        "Test Set: Used to evaluate performance on new data."
      ],
      "metadata": {
        "id": "Ic17adsPHSUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use 80:20 split (most commonly used)."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An imbalanced dataset occurs when one class significantly outnumbers the others (for classification tasks).\n",
        "This can lead to biased models that favor the majority class."
      ],
      "metadata": {
        "id": "Ac1yDRrYHs47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.columns\n"
      ],
      "metadata": {
        "id": "oqIAwnTXJpQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Select numeric features for clustering (excluding categorical/string IDs)\n",
        "features_for_clustering = ['quantity', 'unitprice', 'total_price', 'log_quantity', 'log_unitprice', 'log_total_price']\n",
        "\n",
        "# 2. Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_encoded[features_for_clustering])\n",
        "\n",
        "# 3. Apply KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df_encoded['customer_segment'] = kmeans.fit_predict(X_scaled)"
      ],
      "metadata": {
        "id": "tqusgq7AJJNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Find object columns (text/categorical)\n",
        "cat_cols = df_encoded.select_dtypes(include='object').columns\n",
        "\n",
        "# Label encode all string columns\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))"
      ],
      "metadata": {
        "id": "v03aqNZuKB5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns and prepare train/test\n",
        "X = df_encoded.drop(columns=['customer_segment', 'invoiceno', 'stockcode', 'invoicedate', 'customerid'])\n",
        "y = df_encoded['customer_segment']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "YODOwAl3KHf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Random Forest Classifier, that gave near perfect scores .\n",
        "\n",
        "Metric\t   Score  \n",
        "\n",
        "Accuracy\t 99.9957%\n",
        "Precision \t1.00\n",
        "Recall\t    1.00\n",
        "F1-Score\t  1.00"
      ],
      "metadata": {
        "id": "ewsdmeq6Ka6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
        "top_features = feature_importances.sort_values(ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_features.values, y=top_features.index)\n",
        "plt.title(\"Top 15 Feature Importances\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "61tfNl6lK2DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='Set1', alpha=0.5)\n",
        "plt.title('Customer Segments Visualized via PCA')\n",
        "plt.xlabel('PCA 1')\n",
        "plt.ylabel('PCA 2')\n",
        "plt.colorbar(label='Segment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Cross-validator\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Reduced hyperparameter space for speed\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 150),\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': randint(2, 5),\n",
        "    'min_samples_leaf': randint(1, 3),\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "# Randomized Search setup\n",
        "rf_random = RandomizedSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,  # Reduced for speed\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "print(\"Best Parameters Found:\\n\", rf_random.best_params_)\n",
        "best_rf_model = rf_random.best_estimator_\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "tA5lO8M_VPQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I used RandomizedSearchCV because it is computationally more efficient than GridSearchCV when searching across a wide range of parameters. It randomly samples parameter combinations, allowing faster tuning without exhaustive searches."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Accuracy and other metrics remain extremely high both before and after tuning (~0.99996)\n",
        "\n",
        " Even though performance didn’t improve significantly in scores (since the model was already near-perfect), RandomizedSearchCV gave us a more optimized model with:\n",
        "\n",
        " Fewer estimators (149 trees)\n",
        "\n",
        " Dynamic feature splits\n",
        "\n",
        " Reduced Overfitting Risk: Hyperparameter tuning helps control depth and complexity, especially with real-world noisy data"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation: LightGBM Classifier with Hyperparameter Optimization\n",
        "\n",
        "## Step 1: Import Libraries\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "## Step 2: Set Up Stratified K-Fold CV\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "## Step 3: Define Hyperparameter Space\n",
        "param_dist = {\n",
        "    'n_estimators': randint(100, 300),\n",
        "    'max_depth': [10, 20, 30, -1],\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'num_leaves': randint(20, 150),\n",
        "    'min_child_samples': randint(10, 100),\n",
        "    'subsample': uniform(0.6, 0.4),\n",
        "    'colsample_bytree': uniform(0.6, 0.4)\n",
        "}\n",
        "\n",
        "## Step 4: Randomized Search CV\n",
        "lgbm_random = RandomizedSearchCV(\n",
        "    estimator=LGBMClassifier(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    scoring='accuracy',\n",
        "    cv=cv,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "## Step 5: Fit the Model\n",
        "lgbm_random.fit(X_train, y_train)\n",
        "\n",
        "## Step 6: Evaluate the Best Model\n",
        "print(\"Best Parameters Found:\\n\", lgbm_random.best_params_)\n",
        "best_lgbm = lgbm_random.best_estimator_\n",
        "y_pred = best_lgbm.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "XYvPPcQbYEd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very strong model performance (almost perfect accuracy).\n",
        "\n",
        "LightGBM is slightly behind Random Forest in accuracy by a tiny margin (~0.00003 difference).\n",
        "\n",
        "Training time was faster than Random Forest for this level of accuracy."
      ],
      "metadata": {
        "id": "awjHSXSoatbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the 2 Models\n",
        "\n",
        "| Metric               | **Random Forest**           | **LightGBM**                  |\n",
        "| -------------------- | --------------------------- | ----------------------------- |\n",
        "| **Accuracy**         | 0.99992                     | 0.99993                       |\n",
        "| **Precision**        | 1.00                        | 1.00                          |\n",
        "| **Recall**           | 1.00                        | 1.00                          |\n",
        "| **F1-Score**         | 1.00                        | 1.00                          |\n",
        "| **Confusion Matrix** | Very few misclassifications | Even fewer misclassifications |\n",
        "| **Training Time**    | Moderate                    |  Faster (optimized boosting)\n",
        "| **Model Size**       | Large                       |  Smaller                     |\n",
        "| **Scalability**      | Medium (parallel trees)     |  High (Boosted, GPU support) |\n"
      ],
      "metadata": {
        "id": "91kQWZMUbNv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = ['Random Forest', 'LightGBM']\n",
        "accuracy = [0.99992, 0.99993]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(models, accuracy, color=['green', 'skyblue'])\n",
        "plt.ylim(0.9999, 1.0)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Comparison: Accuracy Score')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "utRx8EJLbzFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_lgbm = lgbm_random.best_estimator_"
      ],
      "metadata": {
        "id": "Docn2_qxcN85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "\n",
        "# Save the trained LightGBM model\n",
        "joblib.dump(best_lgbm, 'lightgbm_final_model.pkl')"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Assume you used a scaler and fit it earlier\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(X_train)\n",
        "\n",
        "# Save the fitted scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n"
      ],
      "metadata": {
        "id": "qOWmLw2Pc5Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "feature_cols = X_train.columns.tolist()\n",
        "\n",
        "with open('feature_columns.json', 'w') as f:\n",
        "    json.dump(feature_cols, f)"
      ],
      "metadata": {
        "id": "3GM9-rLlc_5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_train)\n",
        "\n",
        "joblib.dump(encoder, 'label_encoder.pkl')\n"
      ],
      "metadata": {
        "id": "l4w2FeW-dEbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(best_rf_model, 'random_forest_model.pkl')"
      ],
      "metadata": {
        "id": "55mZS0M4dujI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shopper Spectrum project demonstrated that machine learning can effectively segment e-commerce customers based on their behavior, enabling smarter business decisions. Two powerful models—Random Forest and LightGBM—were trained and tuned using hyperparameter optimization. Among them, LightGBM emerged as the best model with near-perfect accuracy and robust performance on the test set.\n",
        "\n",
        "This segmentation model allows businesses to identify high-value customers, potential churners, and segment-specific behavior, which in turn supports highly personalized marketing strategies, increased retention, and better resource allocation. The deployment-ready code and Streamlit interface make the solution scalable and user-friendly."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}
